/*
 * Software License Agreement (BSD License)
 *
 *  Copyright (c) 2010
 *    Ivan Dryanovski <ivan.dryanovski@gmail.com>
 *    William Morris <morris@ee.ccny.cuny.edu>
 *    St√©phane Magnenat <stephane.magnenat@mavt.ethz.ch>
 *    Radu Bogdan Rusu <rusu@willowgarage.com>
 *
 *  All rights reserved.
 *
 *  Redistribution and use in source and binary forms, with or without
 *  modification, are permitted provided that the following conditions
 *  are met:
 *
 *   * Redistributions of source code must retain the above copyright
 *     notice, this list of conditions and the following disclaimer.
 *   * Redistributions in binary form must reproduce the above
 *     copyright notice, this list of conditions and the following
 *     disclaimer in the documentation and/or other materials provided
 *     with the distribution.
 *   * Neither the name of Willow Garage, Inc. nor the names of its
 *     contributors may be used to endorse or promote products derived
 *     from this software without specific prior written permission.
 *
 *  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 *  "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 *  LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS
 *  FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE
 *  COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT,
 *  INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
 *  BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 *  LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 *  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 *  LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN
 *  ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 *  POSSIBILITY OF SUCH DAMAGE.
 *
 */

#include "kinect_camera/kinect.h"
#include <sensor_msgs/image_encodings.h>
//#include <cv_bridge/CvBridge.h>
#include <cv_bridge/cv_bridge.h> // switches between cv and ros image formats 
#include <boost/make_shared.hpp>
//#include <libfreenect.h>
//#include <libfreenect_sync.h>

namespace kinect_camera
{

const double KinectDriver::SHIFT_SCALE = 0.125;

/** \brief Constructor */
KinectDriver::KinectDriver (ros::NodeHandle comm_nh, ros::NodeHandle param_nh)
    : reconfigure_server_(param_nh),
      width_ (640), height_ (480),
      f_ctx_(NULL), f_dev_(NULL),
      started_(false),
      depth_sent_ (true), rgb_sent_ (true)
{
    // Set up reconfigure server
    ReconfigureServer::CallbackType f = boost::bind(&KinectDriver::configCb, this, _1, _2);
    reconfigure_server_.setCallback(f);

    // Assemble the point cloud data
    std::string kinect_depth_frame;
    param_nh.param ("kinect_depth_frame", kinect_depth_frame, std::string ("/kinect_depth"));
    cloud_.header.frame_id = cloud2_.header.frame_id = kinect_depth_frame;
    cloud_.channels.resize (1);
    cloud_.channels[0].name = "rgb";
    cloud_.channels[0].values.resize (width_ * height_);
    /// @todo "u" and "v" channels?
    cloud_rgb_.channels.resize (1);
    cloud_rgb_.channels[0].name = "rgb";
    cloud_rgb_.channels[0].values.resize (width_ * height_);

    cloud2_.height = height_;
    cloud2_.width = width_;
    cloud2_.fields.resize (4);
    cloud2_.fields[0].name = "x";
    cloud2_.fields[1].name = "y";
    cloud2_.fields[2].name = "z";
    cloud2_.fields[3].name = "rgb";

    // Set all the fields types accordingly
    int offset = 0;
    for (size_t s = 0; s < cloud2_.fields.size (); ++s, offset += 4)
    {
        cloud2_.fields[s].offset   = offset;
        cloud2_.fields[s].count    = 1;
        cloud2_.fields[s].datatype = sensor_msgs::PointField::FLOAT32;
    }

    cloud2_.point_step = offset;
    cloud2_.row_step   = cloud2_.point_step * cloud2_.width;
    cloud2_.data.resize (cloud2_.row_step   * cloud2_.height);
    cloud2_.is_dense = true;  /// @todo THIS IS WRONG!!!! But might be assumed by PCL.

    // Cloud for depth registered to RGB image
    cloud2_rgb_.height = height_;
    cloud2_rgb_.width = width_;
    cloud2_rgb_.fields.resize (4);
    cloud2_rgb_.fields[0].name = "x";
    cloud2_rgb_.fields[1].name = "y";
    cloud2_rgb_.fields[2].name = "z";
    cloud2_rgb_.fields[3].name = "rgb";

    // Set all the fields types accordingly
    offset = 0;
    for (size_t s = 0; s < cloud2_rgb_.fields.size (); ++s, offset += 4)
    {
        cloud2_rgb_.fields[s].offset   = offset;
        cloud2_rgb_.fields[s].count    = 1;
        cloud2_rgb_.fields[s].datatype = sensor_msgs::PointField::FLOAT32;
    }

    cloud2_rgb_.point_step = offset;
    cloud2_rgb_.row_step   = cloud2_rgb_.point_step * cloud2_rgb_.width;
    cloud2_rgb_.data.resize (cloud2_rgb_.row_step   * cloud2_rgb_.height);
    cloud2_rgb_.is_dense = true;

    // Assemble the depth image data
    depth_image_.header.frame_id = kinect_depth_frame;
    depth_image_.height = height_;
    depth_image_.width = width_;
    depth_image_.encoding = "mono16"; //"mono8";
    depth_image_.step = width_ * sizeof(unsigned short);
    depth_image_.data.resize(width_ * height_ * sizeof(unsigned short) );

    // Assemble the image data
    std::string kinect_RGB_frame;
    param_nh.param ("kinect_rgb_frame", kinect_RGB_frame, std::string ("/kinect_rgb"));
    rgb_image_.header.frame_id = rgb_info_.header.frame_id = kinect_RGB_frame;
    rgb_image_.height = height_;
    rgb_image_.width = width_;
    cloud_rgb_.header.frame_id = cloud2_rgb_.header.frame_id = kinect_RGB_frame;

    // Read calibration parameters from disk
    std::string cam_name, rgb_info_url, depth_info_url;
    param_nh.param ("camera_name", cam_name, std::string("camera"));
    param_nh.param ("rgb/camera_info_url", rgb_info_url, std::string("auto"));
    param_nh.param ("depth/camera_info_url", depth_info_url, std::string("auto"));
    if (rgb_info_url.compare("auto") == 0)
        rgb_info_url = std::string("file://")+ros::package::getPath(ROS_PACKAGE_NAME)+std::string("/info/calibration_rgb.yaml");
    if (depth_info_url.compare("auto") == 0)
        depth_info_url = std::string("file://")+ros::package::getPath(ROS_PACKAGE_NAME)+std::string("/info/calibration_depth.yaml");
    ROS_INFO ("[KinectDriver] Calibration URLs:\n\tRGB: %s\n\tDepth: %s",
              rgb_info_url.c_str (), depth_info_url.c_str ());

    rgb_info_manager_   = boost::make_shared<camera_info_manager::CameraInfoManager> (ros::NodeHandle(comm_nh, "rgb"),
                          cam_name, rgb_info_url);
    depth_info_manager_ = boost::make_shared<camera_info_manager::CameraInfoManager> (ros::NodeHandle(comm_nh, "depth"),
                          cam_name, depth_info_url);
    rgb_info_   = rgb_info_manager_->getCameraInfo ();
    depth_info_ = depth_info_manager_->getCameraInfo ();
    rgb_info_.header.frame_id = kinect_RGB_frame;
    depth_info_.header.frame_id = kinect_depth_frame;
    rgb_model_.fromCameraInfo(rgb_info_);
    depth_model_.fromCameraInfo(depth_info_);

    /// @todo Distinguish calibrated/uncalibrated Kinects
    // Read additional calibration parameters
    param_nh.param ("shift_offset", shift_offset_, 1084.0);
    param_nh.param ("projector_depth_baseline", baseline_, 0.075); // 7.5cm

    // Compute transform matrix from (u,v,d) of depth camera to (u,v) of RGB camera
    // From (u,v,d,1) in depth image to (X,Y,Z,W) in depth camera frame
    Eigen::Matrix4d Q;
    Q << 1, 0, 0, -depth_model_.cx(),
      0, 1, 0, -depth_model_.cy(),
      0, 0, 0,  depth_model_.fx(),
      0, 0, 1.0 / baseline_, 0;

    // From (X,Y,Z,W) in depth camera frame to RGB camera frame
    //Eigen::Matrix4d S;
    XmlRpc::XmlRpcValue rot, trans;
    if (param_nh.getParam("depth_rgb_rotation", rot) &&
            param_nh.getParam("depth_rgb_translation", trans) &&
            rot.size() == 9 && trans.size() == 3)
    {
        S << rot[0], rot[1], rot[2], trans[0],
          rot[3], rot[4], rot[5], trans[1],
          rot[6], rot[7], rot[8], trans[2],
          0,      0,      0,      1;
    }
    else
    {
        ROS_WARN("Transform between depth and RGB cameras is not calibrated");
        S << 1, 0, 0, -0.025, // 2.5cm
          0, 1, 0, 0,
          0, 0, 1, 0,
          0, 0, 0, 1;
    }

    // From (X,Y,Z,W) in RGB camera frame to (u,v,w) in RGB image
    Eigen::Matrix<double, 3, 4> P;
    P << rgb_model_.fx(), 0,               rgb_model_.cx(), 0,
      0,               rgb_model_.fy(), rgb_model_.cy(), 0,
      0,               0,               1,               0;

    // Putting it all together, from (u,v,d,1) in depth image to (u,v,w) in RGB image
    depth_to_rgb_ = P*S*Q;

    std::cout << "Transform matrix:" << std::endl << depth_to_rgb_ << std::endl << std::endl;

    // Publishers and subscribers
    image_transport::ImageTransport it(comm_nh);
    pub_rgb_     = it.advertiseCamera ("rgb/image_raw", 1);
    pub_rgb_rect_ = it.advertise("rgb/image_rect_color", 1);
    pub_depth_   = it.advertiseCamera ("depth/image_raw", 1);
    pub_ir_      = it.advertiseCamera ("ir/image_raw", 1);
    pub_depth_points_  = comm_nh.advertise<sensor_msgs::PointCloud> ("depth/points", 15);
    pub_depth_points2_ = comm_nh.advertise<sensor_msgs::PointCloud2>("depth/points2", 15);
    pub_rgb_points_    = comm_nh.advertise<sensor_msgs::PointCloud> ("rgb/points", 15);
    pub_rgb_points2_   = comm_nh.advertise<sensor_msgs::PointCloud2>("rgb/points2", 15);
    pub_imu_ = comm_nh.advertise<sensor_msgs::Imu>("imu", 15);
    
}

/** \brief Initialize a Kinect device, given an index.
  * \param index the index of the device to initialize
  */
bool KinectDriver::init (int index){
    // Initialize the USB
    if (freenect_init (&f_ctx_, NULL) < 0)
    {
        ROS_ERROR ("[KinectDriver::init] Initialization failed!");
        return (false);
    }

    // Get the number of devices available
    int nr_devices = freenect_num_devices (f_ctx_);
    if (nr_devices < 1)
    {
        ROS_WARN ("[KinectDriver::init] No devices found!");
        return (false);
    }
    ROS_DEBUG ("[KinectDriver::init] Number of devices found: %d", nr_devices);
    if (nr_devices <= index)
    {
        ROS_WARN ("[KinectDriver::init] Desired device index (%d) out of bounds (%d)!", index, nr_devices);
        return (false);
    }

    // Open the device
    if (freenect_open_device (f_ctx_, &f_dev_, index) < 0)
    {
        ROS_ERROR ("[KinectDriver::init] Could not open device with index (%d)!", index);
        return (false);
    }

    // Set the appropriate data callbacks
    freenect_set_user(f_dev_, this);
    freenect_set_depth_callback (f_dev_, &KinectDriver::depthCbInternal);
    freenect_set_rgb_callback   (f_dev_, &KinectDriver::rgbCbInternal);
    freenect_set_ir_callback    (f_dev_, &KinectDriver::irCbInternal);

    updateDeviceSettings();

    return (true);
}


void KinectDriver::depthCbInternal (freenect_device *dev, void *buf, uint32_t timestamp)
{
    freenect_depth* tbuf = reinterpret_cast<freenect_depth*>(buf);
    KinectDriver* driver = reinterpret_cast<KinectDriver*>(freenect_get_user(dev));
    driver->depthCb(dev, tbuf, timestamp);
}

void KinectDriver::rgbCbInternal (freenect_device *dev, freenect_pixel *buf, uint32_t timestamp)
{
    KinectDriver* driver = reinterpret_cast<KinectDriver*>(freenect_get_user(dev));
    driver->rgbCb(dev, buf, timestamp);
}

void KinectDriver::irCbInternal (freenect_device *dev, freenect_pixel_ir *buf, uint32_t timestamp)
{
    KinectDriver* driver = reinterpret_cast<KinectDriver*>(freenect_get_user(dev));
    driver->irCb(dev, buf, timestamp);
}

/** \brief Destructor */
KinectDriver::~KinectDriver ()
{
    freenect_close_device (f_dev_);
    freenect_shutdown (f_ctx_);
}

/** \brief Start (resume) the data acquisition process. */
void
KinectDriver::start ()
{
    freenect_start_depth (f_dev_);
    if (config_.color_format == FREENECT_FORMAT_IR)
        freenect_start_ir (f_dev_);
    else
        freenect_start_rgb (f_dev_);

    started_ = true;
}

/** \brief Stop (pause) the data acquisition process. */
void
KinectDriver::stop ()
{
    freenect_stop_depth (f_dev_);
    if (config_.color_format == FREENECT_FORMAT_IR)
        freenect_stop_ir (f_dev_);
    else
        freenect_stop_rgb (f_dev_);

    started_ = false;
}

/** \brief Depth callback. Virtual.
  * \param dev the Freenect device
  * \param buf the resultant output buffer
  * \param timestamp the time when the data was acquired
  */
void
KinectDriver::depthCb (freenect_device *dev, freenect_depth *buf, uint32_t timestamp)
{
    boost::mutex::scoped_lock lock (buffer_mutex_);

    depth_buf_ = buf;
    depth_sent_ = false;

    // Publish only if we have an rgb image too
    if (!rgb_sent_)
        processRgbAndDepth ();
}

/** \brief RGB callback. Virtual.
  * \param dev the Freenect device
  * \param buf the resultant output buffer
  * \param timestamp the time when the data was acquired
  */
void
KinectDriver::rgbCb (freenect_device *dev, freenect_pixel *rgb, uint32_t timestamp)
{
    boost::mutex::scoped_lock lock (buffer_mutex_);

    rgb_buf_ = rgb;
    rgb_sent_ = false;

    // Publish only if we have a depth image too
    if (!depth_sent_) {
        processRgbAndDepth ();
    }
}

void
KinectDriver::irCb (freenect_device *dev, freenect_pixel_ir *rgb, uint32_t timestamp)
{
    boost::mutex::scoped_lock lock (buffer_mutex_);

    // Reusing rgb_image_ for now, hacky as it is
    rgb_buf_ = reinterpret_cast<freenect_pixel*>(rgb);
    rgb_sent_ = false;

    if (!depth_sent_) {
        processRgbAndDepth ();
    }
}

void KinectDriver::processRgbAndDepth()
{
    /// @todo Investigate how well synchronized the depth & color images are

    // Fill raw RGB image message
    if (pub_rgb_.getNumSubscribers () > 0 &&
            config_.color_format != FREENECT_FORMAT_IR)
    {
        // Copy the image data
        memcpy (&rgb_image_.data[0], &rgb_buf_[0], rgb_image_.data.size());

        // Check the camera info
        /// @todo Check this on loading info instead
        if (rgb_info_.height != (size_t)rgb_image_.height || rgb_info_.width != (size_t)rgb_image_.width)
            ROS_DEBUG_THROTTLE (60, "[KinectDriver::rgbCb] Uncalibrated Camera");
    }

    if (pub_ir_.getNumSubscribers () > 0 &&
            config_.color_format == FREENECT_FORMAT_IR)
    {
        /// @todo Publish original 16-bit output? Shifting down to 8-bit for convenience for now
        freenect_pixel_ir *ir = reinterpret_cast<freenect_pixel_ir*>(rgb_buf_);
        for (int i = 0; i < FREENECT_FRAME_PIX; ++i) {
            int val = ir[i];
            rgb_image_.data[i] = (val >> 2) & 0xff;
        }
    }

    bool have_rgb = config_.color_format == FREENECT_FORMAT_RGB;

    // Rectify the RGB image if necessary
    /// @todo Publish rectified RGB image
    cv::Mat rgb_raw(height_, width_, CV_8UC3, rgb_buf_);
    //cv::Mat rgb_rect_;
    if (pub_rgb_rect_.getNumSubscribers () > 0 ||
            pub_depth_points_.getNumSubscribers () > 0 ||
            pub_depth_points2_.getNumSubscribers () > 0 ||
            pub_rgb_points_.getNumSubscribers () > 0 ||
            pub_rgb_points2_.getNumSubscribers () > 0)
    {
        if (have_rgb)
            rgb_model_.rectifyImage(rgb_raw, rgb_rect_);
        else
            ROS_WARN_THROTTLE(60, "Color format must be RGB to publish point clouds");
    }
    double fT = depth_model_.fx() * baseline_;

    /// @todo Implement Z-buffering in RGB space for the depth point clouds
    // Convert the data to ROS format
    if (pub_depth_points_.getNumSubscribers () > 0 && have_rgb)
    {
        // Assemble an ancient sensor_msgs/PointCloud message
        cloud_.points.resize (0); // sensor_msgs/PointCloud is sparse
        cloud_.channels[0].values.resize(0);
        int k = 0;
        for (int v = 0; v < height_; ++v)
        {
            for (int u = 0; u < width_; ++u, ++k)
            {
                double d = SHIFT_SCALE * (shift_offset_ - depth_buf_[k]); // disparity
                if (d <= 0.0)
                    continue;

                // Fill in XYZ
                geometry_msgs::Point32 pt;
                pt.z = fT / d;
                pt.x = ((u - depth_model_.cx()) / depth_model_.fx()) * pt.z;
                pt.y = ((v - depth_model_.cy()) / depth_model_.fy()) * pt.z;
                cloud_.points.push_back(pt);

                // Fill in RGB from corresponding pixel in rectified RGB image
                Eigen::Vector4d uvd1;
                uvd1 << u, v, d, 1;
                Eigen::Vector3d uvw;
                uvw = depth_to_rgb_ * uvd1;
                int u_rgb = uvw[0]/uvw[2] + 0.5;
                int v_rgb = uvw[1]/uvw[2] + 0.5;

                int32_t rgb_packed = 0;
                if (u_rgb >= 0 && u_rgb < width_ && v_rgb >= 0 && v_rgb < height_) {
                    cv::Vec3b rgb = rgb_rect_.at<cv::Vec3b>(v_rgb, u_rgb);
                    rgb_packed = (rgb[0] << 16) | (rgb[1] << 8) | rgb[2];
                }
                cloud_.channels[0].values.push_back(*(float*)(&rgb_packed));
            }
        }
    }

    if (pub_depth_points2_.getNumSubscribers () > 0 && have_rgb)
    {
        // Assemble an awesome sensor_msgs/PointCloud2 message
        float bad_point = std::numeric_limits<float>::quiet_NaN ();
        int k = 0;
        for (int v = 0; v < height_; ++v)
        {
            for (int u = 0; u < width_; ++u, ++k)
            {
                float* pt_data = reinterpret_cast<float*>(&cloud2_.data[0] + k * cloud2_.point_step);
                /// @todo Possible optimization: lookup table mapping shift to depth. Then skip
                // disparity -> XYZW in RGB pixel correspondence.
                double d = SHIFT_SCALE * (shift_offset_ - depth_buf_[k]); // disparity
                if (d <= 0.0) {
                    // not valid
                    pt_data[0] = bad_point;
                    pt_data[1] = bad_point;
                    pt_data[2] = bad_point;
                    // Fill in RGB
                    pt_data[3] = 0;
                    continue;
                }

                // Fill in XYZ
                double Z = fT / d;
                double X = ((u - depth_model_.cx()) / depth_model_.fx()) * Z;
                double Y = ((v - depth_model_.cy()) / depth_model_.fy()) * Z;
                pt_data[0] = X;
                pt_data[1] = Y;
                pt_data[2] = Z;

                // Calculate corresponding pixel in rectified RGB image
                Eigen::Vector4d uvd1;
                uvd1 << u, v, d, 1;
                Eigen::Vector3d uvw;
                uvw = depth_to_rgb_ * uvd1;
                int u_rgb = uvw[0]/uvw[2] + 0.5;
                int v_rgb = uvw[1]/uvw[2] + 0.5;

                // Fill in RGB
                int32_t rgb_packed = 0;
                if (u_rgb >= 0 && u_rgb < width_ && v_rgb >= 0 && v_rgb < height_) {
                    cv::Vec3b rgb = rgb_rect_.at<cv::Vec3b>(v_rgb, u_rgb);
                    rgb_packed = (rgb[0] << 16) | (rgb[1] << 8) | rgb[2];
                }
                memcpy(&pt_data[3], &rgb_packed, sizeof(int32_t));
            }
        }
    }

    if (pub_rgb_points_.getNumSubscribers () > 0 && have_rgb)
    {
        // Assemble an ancient sensor_msgs/PointCloud message
        int16_t MAX_READING = std::numeric_limits<int16_t>::max();
        cv::Mat z_buffer(height_, width_, CV_32SC2, cv::Scalar(MAX_READING, -1));

        cloud_rgb_.points.resize (0); // sensor_msgs/PointCloud is sparse
        cloud_rgb_.channels[0].values.resize(0);
        int k = 0;
        for (int v = 0; v < height_; ++v)
        {
            for (int u = 0; u < width_; ++u, ++k)
            {
                int r = depth_buf_[k]; // raw reading
                double d = SHIFT_SCALE * (shift_offset_ - r); // disparity
                if (d <= 0.0)
                    continue;

                // Compute XYZ in depth camera frame
                double Z = fT / d;
                double X = ((u - depth_model_.cx()) / depth_model_.fx()) * Z;
                double Y = ((v - depth_model_.cy()) / depth_model_.fy()) * Z;

                // Transform to RGB camera frame
                Eigen::Vector4d xyz_depth, xyz_rgb;
                xyz_depth << X, Y, Z, 1;
                xyz_rgb = S * xyz_depth;

                // Project to RGB image
                int u_rgb = (xyz_rgb.x() / xyz_rgb.z()) * rgb_model_.fx() + rgb_model_.cx() + 0.5;
                int v_rgb = (xyz_rgb.y() / xyz_rgb.z()) * rgb_model_.fy() + rgb_model_.cy() + 0.5;
                if (u_rgb < 0 || u_rgb >= width_ || v_rgb < 0 || v_rgb >= height_)
                    continue;

                // Fill in XYZ
                geometry_msgs::Point32 pt;
                pt.x = xyz_rgb.x();
                pt.y = xyz_rgb.y();
                pt.z = xyz_rgb.z();

                // Z buffer check. Replace an old point if the new one is closer, or
                // append the new one if it's at an unmapped pixel.
                cv::Vec2i &reading = z_buffer.at<cv::Vec2i>(v_rgb, u_rgb);
                int32_t rgb_packed = 0;
                /// @todo Get rid of cross-hatching in texture - probably from points that get
                // projected to the same RGB pixel, and we turn one black.
                if (r < reading[0]) {
                    cv::Vec3b rgb = rgb_rect_.at<cv::Vec3b>(v_rgb, u_rgb);
                    rgb_packed = (rgb[0] << 16) | (rgb[1] << 8) | rgb[2];

                    int index = reading[1];
                    if (index >= 0) {
                        int32_t black = 0;
                        cloud_rgb_.channels[0].values[index] = *(float*)(&black);
                    }

                    reading[0] = r;
                }
                reading[1] = cloud_rgb_.points.size();
                cloud_rgb_.points.push_back(pt);
                cloud_rgb_.channels[0].values.push_back(*(float*)(&rgb_packed));
            }
        }
    }

    if (pub_rgb_points2_.getNumSubscribers () > 0 && have_rgb)
    {
        float bad_point = std::numeric_limits<float>::quiet_NaN ();
        float* pt_data = reinterpret_cast<float*>(&cloud2_rgb_.data[0]);
        std::fill_n(pt_data, height_ * width_ * 4, bad_point);
        int k = 0;
        for (int v = 0; v < height_; ++v)
        {
            for (int u = 0; u < width_; ++u, ++k)
            {
                double d = SHIFT_SCALE * (shift_offset_ - depth_buf_[k]); // disparity
                if (d <= 0.0)
                    continue;

                // Compute XYZ in depth camera frame
                double Z = fT / d;
                double X = ((u - depth_model_.cx()) / depth_model_.fx()) * Z;
                double Y = ((v - depth_model_.cy()) / depth_model_.fy()) * Z;

                // Transform to RGB camera frame
                Eigen::Vector4d xyz_depth, xyz_rgb;
                xyz_depth << X, Y, Z, 1;
                xyz_rgb = S * xyz_depth;

                // Project to RGB image
                int u_rgb = (xyz_rgb.x() / xyz_rgb.z()) * rgb_model_.fx() + rgb_model_.cx() + 0.5;
                int v_rgb = (xyz_rgb.y() / xyz_rgb.z()) * rgb_model_.fy() + rgb_model_.cy() + 0.5;
                if (u_rgb < 0 || u_rgb >= width_ || v_rgb < 0 || v_rgb >= height_)
                    continue;

                // Fill in point data
                cv::Vec3b rgb = rgb_rect_.at<cv::Vec3b>(v_rgb, u_rgb);
                int32_t rgb_packed = (rgb[0] << 16) | (rgb[1] << 8) | rgb[2];
                float* pt = pt_data + (v_rgb*width_ + u_rgb)*4;
                // Z buffer check. Fill the point if old Z value is further away *or* NaN.
                if (!(pt[2] < xyz_rgb.z())) {
                    pt[0] = xyz_rgb.x();
                    pt[1] = xyz_rgb.y();
                    pt[2] = xyz_rgb.z();
                    memcpy(&pt[3], &rgb_packed, sizeof(int32_t));
                }
            }
        }
    }

    if (pub_depth_.getNumSubscribers () > 0)
    {
        // Fill in the depth image data
        depthBufferTo8BitImage(depth_buf_);
    }

    publish();
}

void KinectDriver::publish ()
{
    /// @todo Do something with the real timestamps from the device
    // Get the current time
    ros::Time time = ros::Time::now ();
    cloud_.header.stamp = cloud2_.header.stamp = cloud_rgb_.header.stamp = cloud2_rgb_.header.stamp = time;
    rgb_image_.header.stamp   = rgb_info_.header.stamp   = time;
    depth_image_.header.stamp = depth_info_.header.stamp = time;

    // Publish RGB or IR Image
    bool have_rgb = config_.color_format == FREENECT_FORMAT_RGB;
    if (config_.color_format == FREENECT_FORMAT_IR)
    {
        if (pub_ir_.getNumSubscribers() > 0)
            pub_ir_.publish (boost::make_shared<const sensor_msgs::Image> (rgb_image_),
                             boost::make_shared<const sensor_msgs::CameraInfo> (depth_info_));
    }
    else
    {
        if (pub_rgb_.getNumSubscribers () > 0)
            pub_rgb_.publish (boost::make_shared<const sensor_msgs::Image> (rgb_image_),
                              boost::make_shared<const sensor_msgs::CameraInfo> (rgb_info_));
        if (pub_rgb_rect_.getNumSubscribers () > 0 && have_rgb)
        {
            ////////////////////////////////
            // FIXME
            ////////////////////////////////
#if 0 // old
            IplImage ipl = rgb_rect_;
            sensor_msgs::ImagePtr msg_ptr = sensor_msgs::CvBridge::cvToImgMsg(&ipl, "rgb8");
            msg_ptr->header.stamp = time;
            msg_ptr->header.frame_id = rgb_info_.header.frame_id;
            pub_rgb_rect_.publish(msg_ptr);

#else // new - check       
            // convert OpenCV image to ROS message
            // http://www.ros.org/wiki/cv_bridge/Tutorials/UsingCvBridgeToConvertBetweenROSImagesAndOpenCVImages
            // or cv_bridge::CvImagePtr cv_ptr;
            cv_bridge::CvImage cvi;
            cvi.header.stamp = time;
            cvi.header.frame_id = rgb_info_.header.frame_id;
            cvi.encoding = "rgb8";
            cvi.image = rgb_rect_;

            sensor_msgs::Image im;
            cvi.toImageMsg(im);

            pub_rgb_rect_.publish(im);
#endif
        }
    }

    // Publish depth Image
    if (pub_depth_.getNumSubscribers () > 0)
        pub_depth_.publish (boost::make_shared<const sensor_msgs::Image> (depth_image_), boost::make_shared<const sensor_msgs::CameraInfo> (depth_info_));

    // Publish the PointCloud messages
    if (pub_depth_points_.getNumSubscribers () > 0 && have_rgb)
        pub_depth_points_.publish  (boost::make_shared<const sensor_msgs::PointCloud> (cloud_));
    if (pub_depth_points2_.getNumSubscribers () > 0 && have_rgb)
        pub_depth_points2_.publish (boost::make_shared<const sensor_msgs::PointCloud2> (cloud2_));
    if (pub_rgb_points_.getNumSubscribers () > 0 && have_rgb)
        pub_rgb_points_.publish  (boost::make_shared<const sensor_msgs::PointCloud> (cloud_rgb_));
    if (pub_rgb_points2_.getNumSubscribers () > 0 && have_rgb)
        pub_rgb_points2_.publish (boost::make_shared<const sensor_msgs::PointCloud2> (cloud2_rgb_));

    rgb_sent_   = true;
    depth_sent_ = true;
}

void KinectDriver::publishImu()
{
    imu_msg_.header.stamp = ros::Time::now();
    imu_msg_.linear_acceleration.x = accel_x_;
    imu_msg_.linear_acceleration.y = accel_y_;
    imu_msg_.linear_acceleration.z = accel_z_;
    imu_msg_.linear_acceleration_covariance[0] = imu_msg_.linear_acceleration_covariance[4]
            = imu_msg_.linear_acceleration_covariance[8] = 0.01; // @todo - what should these be?
    imu_msg_.angular_velocity_covariance[0] = -1; // indicates angular velocity not provided
    imu_msg_.orientation_covariance[0] = -1; // indicates orientation not provided
    if (pub_imu_.getNumSubscribers() > 0)
        pub_imu_.publish(imu_msg_);
}

void KinectDriver::configCb (Config &config, uint32_t level)
{
    /// @todo Integrate init() in here, so can change device and not worry about first config call

    // Configure color output to be RGB or Bayer
    /// @todo Mucking with image_ here might not be thread-safe
    if (config.color_format == FREENECT_FORMAT_RGB) {
        rgb_image_.encoding = sensor_msgs::image_encodings::RGB8;
        rgb_image_.data.resize(FREENECT_RGB_SIZE);
        rgb_image_.step = FREENECT_FRAME_W * 3;
    }
    else if (config.color_format == FREENECT_FORMAT_BAYER) {
        rgb_image_.encoding = sensor_msgs::image_encodings::BAYER_GRBG8;
        rgb_image_.data.resize(FREENECT_BAYER_SIZE);
        rgb_image_.step = FREENECT_FRAME_W;
    }
    else if (config.color_format == FREENECT_FORMAT_IR) {
        rgb_image_.encoding = sensor_msgs::image_encodings::MONO8;
        rgb_image_.data.resize(FREENECT_BAYER_SIZE);
        rgb_image_.step = FREENECT_FRAME_W;
    }
    else {
        ROS_ERROR("Unknown color format code %d", config.color_format);
    }

    config_ = config;
    updateDeviceSettings();
}

void KinectDriver::updateDeviceSettings()
{
    if (f_dev_) {
        freenect_set_rgb_format(f_dev_, (freenect_rgb_format)config_.color_format);
        freenect_set_led(f_dev_, (freenect_led_options)config_.led);
        freenect_set_tilt_degs(f_dev_, config_.tilt);

        if (started_) {
            if (config_.color_format == FREENECT_FORMAT_IR) {
                freenect_stop_rgb(f_dev_); // just to get rid of log spew
                freenect_start_ir(f_dev_);
            }
            else {
                freenect_stop_ir(f_dev_); // ditto
                freenect_start_rgb(f_dev_);
            }
        }
    }
}

// The formula is Z = T*R/(T - dX), where:
//   - Z is the distance from the camera
//   - T is the baseline between the depth camera and IR projector
//   - R is the distance to the plane of the reference image captured during factory calibration
//   - dX is the calculated pattern shift in the reference plane
// If we assume the reading r is linearly related to dX (e.g. is the disparity) by some scaling
// and offset, we need to fit parameters A and B such that Z = A / (B - r).
// See data points at: http://www.ros.org/wiki/kinect_node

void KinectDriver::depthBufferTo8BitImage(const freenect_depth * buf){
    memcpy((void*)&depth_image_.data[0], (void*)buf, width_*height_*sizeof(unsigned short));
    return;
}

} // namespace kinect_camera
